# vi: set ft=ruby :
num_nodes = ENV["NUM_NODES"] || 2
nomad_unit = <<-EOF
[Service]
ExecStart=/usr/bin/nomad agent -config /etc/nomad
[Install]
WantedBy=multi-user.target
EOF

nomad = <<-EOF
if [ -a /usr/bin/nomad ]; then
  echo nomad is already installed
else
  apt-get install -y unzip
  cd /tmp
  curl https://releases.hashicorp.com/nomad/0.5.2/nomad_0.5.2_linux_amd64.zip > nomad.zip
  unzip -d . nomad.zip
  chmod +x nomad
  mv nomad /usr/bin/nomad
  rm nomad.zip
  useradd -d /var/lib/nomad nomad
  usermod -a -G nomad ubuntu
  mkhomedir_helper nomad
  chmod g+rw /var/lib/nomad
  mkdir -p /etc/nomad
  mkdir -p /usr/lib/systemd/system
  echo '#{nomad_unit}' > /usr/lib/systemd/system/nomad.service
  systemctl daemon-reload
  systemctl unmask nomad
  systemctl enable nomad
  sudo -u ubuntu echo "export NOMAD_ADDR=http://192.168.59.101:4646" >> /home/ubuntu/.bash_profile
fi
EOF

docker_daemon_config = %q{'{"cluster-store":"consul://127.0.0.1:8500"}'}
docker = <<-EOF
count=$(apt list --installed | grep docker | wc -l)
if [ $count == 0 ]; then
  curl -fsSL https://get.docker.com/ | sh
  mkdir -p /etc/docker
  echo #{docker_daemon_config} > /etc/docker/daemon.json
  systemctl enable docker
  systemctl start docker
  usermod -a -G docker ubuntu
else
  echo docker is already installed
fi
EOF

consul_initial = <<-EOF
count=$(docker ps -q --filter name=consul | wc -l)
if [ $count == 0 ]; then
  docker run -d --name consul --network=host --restart always consul agent -server -bind 192.168.59.101 -bootstrap
fi

# wait for consul to come up
while ! curl -s localhost:8500 > /dev/null; do sleep 1; done
echo "Consul is available!"
EOF

consul_joiner = <<-EOF
count=$(docker ps -q --filter name=consul | wc -l)
if [ $count == 0 ]; then
 docker run -d --network=host --name consul --restart always consul agent -server -bind $IP -join 192.168.59.101
fi

# wait for consul to come up
while ! curl -s localhost:8500 > /dev/null; do sleep 1; done
echo "Consul is available!"
EOF

weave = <<-EOF
if [ -a /usr/bin/weave ]; then
  echo weave is already installed
else
  curl -L https://git.io/weave > /usr/bin/weave
  chmod +x /usr/bin/weave
fi
EOF

scope = <<-EOF
if [ -a /usr/bin/scope ]; then
  echo scope is already installed
else
  curl -L https://git.io/scope > /usr/bin/scope
  chmod +x /usr/bin/scope
fi
EOF

launch_weave = <<-EOF
count=$(docker ps -q -f name=weave$ | wc -l)
if [ $count -eq 0 ]; then
  /usr/bin/weave launch --ipalloc-init consensus=3
else
  echo weave is already running
  exit 0
fi
EOF

launch_scope = <<-EOF
count=$(docker ps -q -f name=weavescope | wc -l)
if [ $count -eq 0 ]; then
  if [ -z "$SCOPE_TOKEN" ]; then
    /usr/bin/scope launch
  else
    /usr/bin/scope launch --service-token=$SCOPE_TOKEN
  fi
else
  echo scope is already running
  exit 0
fi
EOF

create_networks = <<-EOF
for network in internal external secure backoffice; do
  if [ $(docker network ls | grep $network | wc -l) -eq 0 ]
  then
    docker network create -d weave $network
  else
    echo docker network $network already created
  fi
done
EOF

Vagrant.configure(2) do |config|

  config.vm.provider "virtualbox" do |vb, override|
    override.vm.box = "ubuntu/xenial64"
    vb.memory = "6144"
    $interface_name = "enp0s8"
  end

  config.vm.provider :aws do |aws, override|
    override.vm.box = "aws_dummy_box"
    override.vm.box_url = "https://github.com/mitchellh/vagrant-aws/raw/master/dummy.box"

    override.ssh.username = "ubuntu"
    override.ssh.private_key_path = "~/.ssh/nomad.pem"
    override.nfs.functional = false

    aws.ami = "ami-2830f947"
    aws.keypair_name = "microservices-demo-nomad"
    aws.instance_type = "m4.large"
    aws.region = "eu-central-1"
    aws.availability_zone = "eu-central-1a"
    aws.security_groups = [ENV["AWS_SECURITY_GROUP_ID"]]
    aws.subnet_id = ENV["AWS_SUBNET_ID"]
    aws.associate_public_ip = true
    $interface_name = "ens3"
  end

  config.vm.provision "shell", inline: nomad
  config.vm.provision "shell", inline: docker
  config.vm.provision "shell", inline: "sudo sysctl -w vm.max_map_count=262144"
  config.vm.provision "shell", inline: weave
  config.vm.provision "shell", inline: scope


  (1..num_nodes.to_i).each do |n|
    ip = "192.168.59.10#{n}"
    name = "ci-sockshop-nomad-node#{n}"
    nomad_config = <<-EOF
    bind_addr = "#{ip}"
    data_dir = "/var/lib/nomad"
    EOF

    if name == "ci-sockshop-nomad-node1"
      nomad_config << <<-EOF
      server {
        enabled = true
        bootstrap_expect = 1
      }
      EOF
    else
      nomad_config << <<-EOF
      client {
        enabled = true
        network_interface = "#{$interface_name}"
        servers = ["192.168.59.101"]
        options = {
          driver.raw_exec.enable = "1"
          docker.cleanup.image = false
        }
      }
      EOF
    end


    config.vm.define name do |m|
      m.vm.hostname = name
      m.vm.network "private_network", ip: ip

      m.vm.provider :aws do |aws, override|
        aws.associate_public_ip = true
        aws.private_ip_address = ip
        aws.tags = {'Name' => name}
      end

      if name == "ci-sockshop-nomad-node1"
        m.vm.provision "shell", name: "Start consul (bootstrapped)", inline: consul_initial
      else
        m.vm.provision "shell", name: "Start consul (join cluster)", env: {"IP" => ip}, inline: consul_joiner
      end

      m.vm.provision "shell", name: "restart docker", inline: "systemctl restart docker"
      m.vm.provision "shell", name: "nomad config", inline: "echo '#{nomad_config}' > /etc/nomad/config.hcl"
      m.vm.provision "shell", name: "Start nomad", inline: "systemctl start nomad"
      m.vm.provision "shell", name: "Launch weave", inline: launch_weave
      m.vm.provision "shell", name: "Launch scope", env: {"SCOPE_TOKEN" => ENV["SCOPE_TOKEN"] || ""}, inline: launch_scope

      # Stuff only the head node should take care of
      if name == "ci-sockshop-nomad-node1"
        m.vm.provider "virtualbox" do |vb|
          vb.memory = "1024"
        end

        m.vm.provision "shell", name: "Create weave networks", inline: create_networks
        m.vm.provision "file", source: "./jobs/netman.nomad", destination: "/home/ubuntu/netman.nomad"
        m.vm.provision "file", source: "./jobs/weavedemo.nomad", destination: "/home/ubuntu/weavedemo.nomad"
        m.vm.provision "file", source: "./jobs/logging-elk.nomad", destination: "/home/ubuntu/logging-elk.nomad"
        m.vm.provision "file", source: "./jobs/logging-fluentd.nomad", destination: "/home/ubuntu/logging-fluentd.nomad"
      else
        # Stuff the rest of the nodes should take care of
        m.vm.provision "shell", inline: "/usr/bin/weave connect 192.168.59.101"
        m.vm.provision "file", source: "./scripts/netman.sh", destination: "/tmp/netman"
        m.vm.provision "shell", inline: "mv /tmp/netman /usr/bin/netman"
      end
    end
  end # 1..3
end # Vagrantfile
